from typing import List, Optional
from pydantic import BaseModel, Field
import langgraph.graph
from langchain_openai import ChatOpenAI
from tavily import TavilyClient
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize Tavily client
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

# Initialize LLM
llm = ChatOpenAI(model="gpt-4.1", temperature=0)

class Document(BaseModel):
    """Represents a document with title, URL, and content."""
    title: str
    url: str
    content: str

class ResearchState(BaseModel):
    """State for the research workflow."""
    user_research_task: str = Field(description="What the user is looking for")
    list_of_searches: List[str] = Field(default_factory=list, description="Things that have been searched for")
    relevant_documents: List[Document] = Field(default_factory=list, description="Documents deemed most relevant")
    new_documents: List[Document] = Field(default_factory=list, description="Documents returned from search, not necessarily most relevant")
    still_missing: Optional[str] = Field(default=None, description="Information we still need to look for")
    answer: Optional[str] = Field(default=None, description="The answer to the research task")

class SearchQuery(BaseModel):
    """Search query generated by LLM."""
    query: str = Field(description="The search query to execute")

class ReviewResult(BaseModel):
    """Result from the review node."""
    relevant_document_indexes: List[int] = Field(description="Indexes of the 5 most relevant documents")
    still_missing: Optional[str] = Field(default=None, description="Information still missing, if any")

class Summary(BaseModel):
    """Final summary answer."""
    answer: str = Field(description="The answer to the research task, under 100 words")

def format_documents_for_prompt(documents: List[Document], max_content_length: int = 500) -> str:
    if not documents:
        return "No documents available."
    
    formatted_text = ""
    for i, doc in enumerate(documents):
        formatted_text += f"\nDocument {i}:\nTitle: {doc.title}\nURL: {doc.url}\nContent: {doc.content[:max_content_length]}...\n"
    
    return formatted_text

def search_node(state: ResearchState) -> dict:
    """
    Search node: LLM creates query string, sends to Tavily, adds results to new_documents.
    """
    print(f"\n\033[1;36m=== SEARCH NODE ===\033[0m")  # Bold cyan
    print(f"\033[1mResearch Task:\033[0m {state.user_research_task}...")
    print(f"\033[1mPrevious searches:\033[0m {state.list_of_searches}")
    print(f"\033[1mStill missing:\033[0m {state.still_missing}")
    
    # Create structured LLM for search query generation
    structured_llm = llm.with_structured_output(SearchQuery)
    
    # Generate search query
    if state.still_missing:
        prompt = f"""
        The user is researching: {state.user_research_task}
        
        We still need to find information about: {state.still_missing}
        
        Create a search query to find this missing information. Make it specific and focused.
        """
    else:
        prompt = f"""
        The user is researching: {state.user_research_task}
        
        Create a search query to find relevant information. Make it specific and focused.
        """
    
    search_result = structured_llm.invoke(prompt)
    query = search_result.query
    
    print(f"\033[1mGenerated query:\033[0m {query}")
    
    # Search with Tavily
    response = tavily_client.search(query)
    results = response.get('results', [])
    
    # Convert results to Document objects
    new_docs = []
    for result in results:
        doc = Document(
            title=result.get('title', 'No title'),
            url=result.get('url', 'No URL'),
            content=result.get('content', 'No content')
        )
        new_docs.append(doc)
    
    # Update state
    return {
        "new_documents": new_docs,
        "list_of_searches": state.list_of_searches + [query]
    }

def review_node(state: ResearchState) -> dict:
    """
    Review node: LLM reviews documents and returns relevant indexes and missing info.
    """
    print(f"\n\033[1;33m=== REVIEW NODE ===\033[0m")  # Bold yellow
    print(f"\033[1mResearch Task:\033[0m {state.user_research_task}...")
    print(f"\033[1mStill missing:\033[0m {state.still_missing}")
    
    # Combine all documents
    all_documents = state.relevant_documents + state.new_documents
    
    if not all_documents:
        raise ValueError("No documents after search.")
    
    print(f"\033[1mDocuments preview:\033[0m {format_documents_for_prompt(all_documents, max_content_length=100)}")
    
    # Create structured LLM for review
    structured_llm = llm.with_structured_output(ReviewResult)
    
    # Create prompt for LLM to review documents
    documents_text = format_documents_for_prompt(all_documents, max_content_length=3000)
    
    prompt = f"""
    Research Task: {state.user_research_task}
    
    Available Documents:{documents_text}
    
    Please analyze these documents and:
    1. Select the 5 most relevant documents (by index number 0-{len(all_documents)-1})
    2. Determine if any information is still missing from the research task
    
    If nothing critical is missing, then still_missing to None.
    """
    
    review_result = structured_llm.invoke(prompt)
    
    print(f"\033[1mSelected document indexes:\033[0m {review_result.relevant_document_indexes}")
    print(f"\033[1mStill missing:\033[0m {review_result.still_missing}")
    
    # Get the selected documents
    selected_documents = [all_documents[i] for i in review_result.relevant_document_indexes if i < len(all_documents)]
    
    return {
        "relevant_documents": selected_documents,
        "still_missing": review_result.still_missing,
        "new_documents": []  # Clear new documents
    }

def summarize_node(state: ResearchState) -> dict:
    """
    Summarize node: Creates final answer if nothing is missing.
    """
    print(f"\n\033[1;32m=== SUMMARIZE NODE ===\033[0m")  # Bold green
    print(f"\033[1mResearch Task:\033[0m {state.user_research_task}...")
    print(f"\033[1mTotal searches performed:\033[0m {len(state.list_of_searches)}")
    
    # Create structured LLM for summary generation
    structured_llm = llm.with_structured_output(Summary)
    
    # Create documents text for the prompt
    documents_text = format_documents_for_prompt(state.relevant_documents, max_content_length=3000)
    
    prompt = f"""
    Research Task: {state.user_research_task}
    
    Based on the following relevant documents:{documents_text}
    
    Provide a comprehensive answer to the research task. Your answer should:
    - Be accurate and well-supported by the documents
    - Be concise (under 100 words)
    - Address the research question directly
    - Include key findings and insights
    """
    
    summary_result = structured_llm.invoke(prompt)
    print(f"\033[1mGenerated answer:\033[0m {summary_result.answer[:200]}...")
    return {
        "answer": summary_result.answer
    }

def should_continue(state: ResearchState) -> str:
    """
    Determines whether to continue searching or summarize.
    """
    if state.still_missing is None:
        return "summarize"
    else:
        return "search"

def build_research_graph():
    """
    Builds and returns the compiled research graph.
    """
    # Create the state graph
    workflow = langgraph.graph.StateGraph(ResearchState)
    
    # Add nodes
    workflow.add_node("search", search_node)
    workflow.add_node("review", review_node)
    workflow.add_node("summarize", summarize_node)
    
    # Add edges
    workflow.add_edge(langgraph.graph.START, "search")
    workflow.add_edge("search", "review")
    workflow.add_conditional_edges(
        "review",
        should_continue,
        {
            "search": "search",
            "summarize": "summarize"
        }
    )
    workflow.add_edge("summarize", langgraph.graph.END)
    
    # Compile the graph
    return workflow.compile()

def run_research(user_task: str) -> ResearchState:
    """
    Runs the research workflow with the given user task.
    
    Args:
        user_task: The research question or task
        
    Returns:
        ResearchState: The final state with the answer
    """
    # Create initial state
    initial_state = ResearchState(user_research_task=user_task)
    
    # Build and run the graph
    graph = build_research_graph()
    result = graph.invoke(initial_state)
    
    # Convert result back to Pydantic model
    return ResearchState(**result)

if __name__ == "__main__":
    # Example usage
    research_question = "What is the animal in the logo of the basketball team from the city where the voice actor behind Homer Simpson was born?"
    result = run_research(research_question)
    print(f"Research Task: {result.user_research_task}")
    print(f"Answer: {result.answer}")
    print(f"Documents Found: {len(result.relevant_documents)}") 